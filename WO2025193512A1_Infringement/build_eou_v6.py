"""
EoU v6 — Clean but complete. Full sentences, all source links, professional.
No page numbers or arrows, but proper formatting and clear language.
"""

from pptx import Presentation
from pptx.util import Inches, Pt
from pptx.dml.color import RGBColor
from pptx.enum.text import PP_ALIGN
from pptx.enum.shapes import MSO_SHAPE
import os

OUTPUT_DIR = os.path.dirname(os.path.abspath(__file__))
PATENT = "WO/2025/193512A1"

BLACK = RGBColor(0, 0, 0)
DARK = RGBColor(35, 35, 45)
GRAY = RGBColor(80, 80, 90)
BLUE = RGBColor(0, 60, 160)
LIGHT_BG = RGBColor(250, 250, 252)


def add_text(slide, left, top, w, h, text, size=11, bold=False, color=BLACK, italic=False):
    box = slide.shapes.add_textbox(Inches(left), Inches(top), Inches(w), Inches(h))
    tf = box.text_frame
    tf.word_wrap = True
    p = tf.paragraphs[0]
    p.text = text
    p.font.size = Pt(size)
    p.font.bold = bold
    p.font.italic = italic
    p.font.color.rgb = color
    return box


def add_multiline(slide, left, top, w, h, lines):
    """lines = list of (text, size, color, bold, italic)"""
    box = slide.shapes.add_textbox(Inches(left), Inches(top), Inches(w), Inches(h))
    tf = box.text_frame
    tf.word_wrap = True
    for i, (text, sz, clr, bld, ital) in enumerate(lines):
        if i == 0:
            p = tf.paragraphs[0]
        else:
            p = tf.add_paragraph()
        p.text = text
        p.font.size = Pt(sz)
        p.font.color.rgb = clr
        p.font.bold = bld
        p.font.italic = ital
        p.space_after = Pt(4)
    return box


def screenshot_box(slide, left, top, w, h):
    shape = slide.shapes.add_shape(MSO_SHAPE.RECTANGLE, Inches(left), Inches(top), Inches(w), Inches(h))
    shape.fill.solid()
    shape.fill.fore_color.rgb = LIGHT_BG
    shape.line.color.rgb = RGBColor(215, 215, 220)
    shape.line.width = Pt(0.75)


def build(filename, company, product, slides_data):
    prs = Presentation()
    prs.slide_width = Inches(10)
    prs.slide_height = Inches(7.5)

    # ============ TITLE SLIDE ============
    sl = prs.slides.add_slide(prs.slide_layouts[6])

    add_text(sl, 0.5, 0.5, 9, 0.35, f"Evidence of Use: {PATENT}", size=13, bold=True, color=DARK)
    add_text(sl, 0.5, 0.95, 9, 0.3, "Single Shot 3D Modelling from 2D Image", size=11, color=GRAY)

    add_text(sl, 0.5, 1.6, 9, 0.5, company, size=22, bold=True, color=DARK)
    add_text(sl, 0.5, 2.15, 9, 0.35, product, size=12, color=GRAY)

    # Claim 1
    add_text(sl, 0.5, 2.85, 9, 0.3, "Independent Claim 1", size=11, bold=True, color=DARK)
    claim1 = (
        '"A method for generating a 3D model comprising: obtaining a 2D image of an object; '
        'classifying the object in the image; segmenting the object from the image; '
        'dimensionally sampling the segmented image of the object; extracting texture information '
        'from the segmented image; generating a 3D mesh model based at least on the dimensional sampling; '
        'and rendering the texture information onto the 3D mesh model to create a 3D representation of the object."'
    )
    add_text(sl, 0.5, 3.2, 9, 1.4, claim1, size=10, color=GRAY, italic=True)

    # Dependent claims
    add_text(sl, 0.5, 4.75, 9, 0.3, "Dependent Claims", size=11, bold=True, color=DARK)
    deps_lines = [
        ("Claim 5: The 3D mesh model is generated by a trained neural network based on the dimensional samples.", 9, GRAY, False, False),
        ("Claim 7: The texture information represents a primary face of the object.", 9, GRAY, False, False),
        ("Claim 8: The primary face texture is rendered on opposite sides of the 3D mesh model.", 9, GRAY, False, False),
        ("Claim 9: Manipulating the 3D representation to provide multiple different views of the object.", 9, GRAY, False, False),
        ("Claim 10: The 3D representation is manipulated based on user input.", 9, GRAY, False, False),
        ("Claim 11: Capturing a 2D image of each manipulation of the 3D representation.", 9, GRAY, False, False),
    ]
    add_multiline(sl, 0.5, 5.1, 9, 1.8, deps_lines)

    add_text(sl, 0.5, 7.0, 5, 0.25, "Assignee: Carnegie Mellon University  |  Inventor: Marios Savvides et al.", size=9, color=RGBColor(140, 140, 140))

    # ============ ELEMENT SLIDES ============
    for sd in slides_data:
        sl = prs.slides.add_slide(prs.slide_layouts[6])

        # Claim language header
        add_text(sl, 0.4, 0.35, 9.2, 0.6, sd["claim_language"], size=12, bold=True, color=DARK)

        # Mapping section
        y = 1.1
        for claim_phrase, product_evidence in sd["mapping"]:
            # Claim phrase in quotes
            add_text(sl, 0.4, y, 5.6, 0.3, f'"{claim_phrase}"', size=10, bold=True, color=DARK)
            # Product evidence as full sentence
            add_text(sl, 0.4, y + 0.32, 5.6, 0.6, product_evidence, size=10, color=GRAY)
            y += 1.0

        # Screenshot boxes on right
        screenshot_box(sl, 6.2, 1.1, 3.4, 2.0)
        screenshot_box(sl, 6.2, 3.3, 3.4, 1.8)

        # Sources section
        src_y = max(y + 0.15, 5.3)
        add_text(sl, 0.4, src_y, 9.2, 0.25, "Evidence Sources", size=10, bold=True, color=DARK)
        for j, src in enumerate(sd["sources"]):
            add_text(sl, 0.4, src_y + 0.3 + j * 0.25, 9.2, 0.25, src, size=9, color=BLUE)

        # Commentary
        if "commentary" in sd and sd["commentary"]:
            comm_y = src_y + 0.3 + len(sd["sources"]) * 0.25 + 0.15
            add_text(sl, 0.4, comm_y, 9.2, 0.7, sd["commentary"], size=9, color=GRAY, italic=True)

    path = os.path.join(OUTPUT_DIR, filename)
    prs.save(path)
    print(f"{filename} created ({len(slides_data) + 1} slides)")


# =====================================================================
# GOOGLE
# =====================================================================
build("EoU_Google_Shopping.pptx", "Google (Alphabet)", "Google Shopping 3D Product Viewer", [
    {
        "claim_language": "Obtaining a 2D image, classifying the object, and segmenting the object from the image",
        "mapping": [
            ("obtaining a 2D image of an object",
             "Google's 3D product generation pipeline accepts as few as one to three product images as input. This is documented in their research blog describing the Veo-based generation system."),
            ("classifying the object in the image",
             "Google Merchant Center requires sellers to assign a product category before 3D generation. The Veo model was fine-tuned on millions of assets organized by product category, including shoes, furniture, electronics, and apparel."),
            ("segmenting the object from the image",
             "Google's research blog explicitly states that their pipeline includes 'removing unwanted backgrounds' as a processing step, which constitutes object segmentation."),
        ],
        "sources": [
            "https://research.google/blog/bringing-3d-shoppable-products-online-with-generative-ai/",
            "https://support.google.com/merchants/answer/6324436",
        ],
        "commentary": "Google's own technical blog publicly documents both the background removal step and the category-based training approach.",
    },
    {
        "claim_language": "Dimensionally sampling and generating a 3D mesh model (Claim 1) using a trained neural network (Claim 5)",
        "mapping": [
            ("dimensionally sampling the segmented image of the object",
             "Google's pipeline extracts spatial information across three documented generations: Gen 1 predicts '3D priors' and estimates 'camera positions from sparse images'; Gen 2 uses Score Distillation Sampling to optimize 3D model parameters; Gen 3 fine-tunes Veo on assets rendered from various camera angles."),
            ("generating a 3D mesh model based at least on the dimensional sampling",
             "All three generations produce 3D representations from the extracted spatial data, including NeRF-based models, optimized meshes, and volumetric representations. Google also supports merchant-uploaded glTF and glb files, which are mesh formats."),
            ("the 3D mesh model is generated by a trained neural network",
             "The Veo model is a video diffusion neural network. NeRF is a neural radiance field network. Score Distillation Sampling uses a neural diffusion model. All three generations are neural network-based systems."),
        ],
        "sources": [
            "https://research.google/blog/bringing-3d-shoppable-products-online-with-generative-ai/",
        ],
        "commentary": "The research blog contains detailed pipeline diagrams for each of the three generations, documenting the neural network architecture and dimensional extraction process.",
    },
    {
        "claim_language": "Extracting and rendering texture (Claim 1), with multiple views based on user input (Claims 9, 10, 11)",
        "mapping": [
            ("extracting texture information from the segmented image",
             "Google's blog explicitly states that Veo captures 'complex interactions between light, material, texture, and geometry.' The word 'texture' appears directly in their technical description of the system's capabilities."),
            ("rendering the texture information onto the 3D mesh model to create a 3D representation",
             "The final output is a photorealistic, textured 360-degree product spin that is rendered and displayed on Google Shopping product pages."),
            ("manipulating the 3D representation to provide multiple different views, based on user input, and capturing a 2D image of each manipulation",
             "The interactive 360-degree viewer allows users to drag and rotate the product to view it from any angle. Google reports 50% higher engagement on products with 3D imagery. Each frame of the 360-degree spin constitutes a 2D capture of the 3D representation at that viewpoint."),
        ],
        "sources": [
            "https://research.google/blog/bringing-3d-shoppable-products-online-with-generative-ai/",
            "https://support.google.com/merchants/answer/13671720",
        ],
        "commentary": "The research blog explicitly uses the word 'texture' in describing the system. The interactive 360-degree viewer simultaneously satisfies Claims 9, 10, and 11.",
    },
])


# =====================================================================
# MESHY
# =====================================================================
build("EoU_Meshy_AI.pptx", "Meshy Inc.", "Meshy AI Image-to-3D Generator", [
    {
        "claim_language": "Obtaining a 2D image, classifying the object, and segmenting the object from the image",
        "mapping": [
            ("obtaining a 2D image of an object",
             "Meshy's Image-to-3D feature accepts a single image upload as the primary input method. Users upload one image to generate a complete 3D model."),
            ("classifying the object in the image",
             "The system generates coherent front, side, and back views from the single input image. This multi-view prediction requires the system to understand what type of object it is processing, as predicting unseen angles of a shoe differs from predicting unseen angles of a chair."),
            ("segmenting the object from the image",
             "All output 3D models contain only the target object with no background elements. The object must be isolated from the scene before mesh generation occurs, as evidenced by the clean outputs."),
        ],
        "sources": [
            "https://www.meshy.ai/",
            "https://www.meshy.ai/features/image-to-3d",
        ],
        "commentary": "The clean isolated output proves segmentation occurred. The coherent multi-view generation demonstrates object classification capability.",
    },
    {
        "claim_language": "Dimensionally sampling and generating a 3D mesh model (Claim 1) using a trained neural network (Claim 5)",
        "mapping": [
            ("dimensionally sampling the segmented image of the object",
             "Meshy's Smart Remesh feature allows users to adjust the polygon count from 1,000 to 300,000 triangles or quads. This configurable mesh resolution demonstrates dimensional sampling at different granularities."),
            ("generating a 3D mesh model based at least on the dimensional sampling",
             "Meshy exports 3D models in seven mesh formats: FBX, GLB, OBJ, STL, 3MF, USDZ, and BLEND. These are all standard polygon mesh formats defined by vertices, faces, and edges. There is no ambiguity that the output is a 3D mesh model."),
            ("the 3D mesh model is generated by a trained neural network",
             "Meshy's entire pipeline is AI and neural network-based. The Image-to-3D feature uses trained models to predict 3D geometry from 2D input."),
        ],
        "sources": [
            "https://www.meshy.ai/",
        ],
        "commentary": "This is the strongest match for claim element (f) in the portfolio. OBJ, FBX, GLB, and STL are mesh formats by definition, eliminating any design-around argument.",
    },
    {
        "claim_language": "Extracting and rendering texture (Claim 1), including primary face texture on opposite sides (Claims 7, 8)",
        "mapping": [
            ("extracting texture information from the segmented image",
             "Meshy generates full PBR (Physically Based Rendering) texture maps including Diffuse, Roughness, Metallic, and Normal maps. This is a comprehensive texture extraction pipeline."),
            ("rendering the texture information onto the 3D mesh model",
             "The PBR textures are applied to the mesh geometry and displayed in Meshy's browser-based 3D viewer. Users can download the model with textures baked onto the mesh."),
            ("the texture information represents a primary face of the object, rendered on opposite sides of the 3D mesh model",
             "The input image captures the front or primary face of the object. When the back of the object is not visible in the input image, Meshy infers or mirrors the primary face texture to the opposite side of the mesh."),
        ],
        "sources": [
            "https://www.meshy.ai/",
            "https://www.meshy.ai/features/ai-texturing",
        ],
        "commentary": "The PBR texture pipeline with Diffuse, Roughness, Metallic, and Normal maps constitutes comprehensive texture extraction. Claims 7 and 8 are directly observable in the output.",
    },
    {
        "claim_language": "Manipulating the 3D representation for multiple views based on user input, and capturing 2D images (Claims 9, 10, 11)",
        "mapping": [
            ("manipulating the 3D representation to provide multiple different views of the object",
             "The browser-based 3D viewer allows the model to be viewed from any angle. Meshy also automatically generates front, side, and back view images from the 3D model."),
            ("the 3D representation is manipulated based on user input",
             "Users drag to rotate the model, scroll to zoom, and click to inspect different parts of the 3D object interactively."),
            ("capturing a 2D image of each manipulation of the 3D representation",
             "The export functionality allows users to capture 2D renders from any viewpoint. The multi-format download includes rendered views of the model."),
        ],
        "sources": [
            "https://www.meshy.ai/",
        ],
        "commentary": "The interactive 3D viewer combined with automatic multi-view generation and export functionality satisfies all three dependent claims.",
    },
])


# =====================================================================
# APPLE
# =====================================================================
build("EoU_Apple_SHARP.pptx", "Apple Inc.", "SHARP + Spatial Scenes (iOS 26)", [
    {
        "claim_language": "Obtaining a 2D image, segmenting the object, and dimensionally sampling the segmented image",
        "mapping": [
            ("obtaining a 2D image of an object",
             "SHARP is designed for single-image input. Apple's documentation states: 'Given a single photograph, it regresses the parameters of a 3D Gaussian representation.' The command line interface is simply: sharp predict -i image_path.png"),
            ("segmenting the object from the image",
             "SHARP uses Apple's Depth Pro for monocular depth estimation, which separates foreground objects from background by assigning different depth values. Per-pixel 3D predictions require understanding object boundaries."),
            ("dimensionally sampling the segmented image of the object",
             "SHARP predicts metric-scale 3D Gaussian positions with 'absolute scale, supporting metric camera movements.' Each Gaussian has position, scale, and rotation parameters, which constitute dimensional data extracted from the 2D image."),
        ],
        "sources": [
            "https://github.com/apple/ml-sharp",
            "https://machinelearning.apple.com/research/sharp-monocular-view",
        ],
        "commentary": "All elements are documented in Apple's GitHub repository and machine learning research page.",
    },
    {
        "claim_language": "Extracting texture, generating a 3D model, and rendering (Claim 1), using a trained neural network (Claim 5)",
        "mapping": [
            ("extracting texture information from the segmented image",
             "Each 3D Gaussian includes spherical harmonic appearance coefficients that encode color and view-dependent appearance extracted from the input image."),
            ("generating a 3D mesh model based at least on the dimensional sampling",
             "IMPORTANT: SHARP outputs 3D Gaussian splats in .ply format, which is NOT a traditional polygon mesh. This is a potential claim construction issue. Gaussian splats represent the scene as point-based primitives rather than vertices, faces, and edges."),
            ("rendering the texture information onto the 3D mesh model to create a 3D representation",
             "SHARP renders photorealistic novel views at over 100 frames per second on a standard GPU, with 'high-resolution rendering with sharp details and fine structures.'"),
            ("the 3D mesh model is generated by a trained neural network",
             "The entire pipeline is a single forward pass through a trained neural network. No iterative optimization is required."),
        ],
        "sources": [
            "https://machinelearning.apple.com/research/sharp-monocular-view",
            "https://huggingface.co/papers/2512.10685",
        ],
        "commentary": "The key vulnerability is element (f): Gaussian splats are functionally equivalent to meshes but mathematically distinct. Patent counsel should evaluate the claim construction argument.",
    },
    {
        "claim_language": "Multiple views and user input manipulation (Claims 9, 10), plus commercial deployment",
        "mapping": [
            ("manipulating the 3D representation to provide multiple different views of the object",
             "SHARP renders novel views from arbitrary nearby viewpoints. The Splat Studio app on Apple Vision Pro enables full 3D scene exploration from the generated representation."),
            ("the 3D representation is manipulated based on user input",
             "On Vision Pro, the user physically moves their head and body to change the viewpoint. In Splat Studio, users navigate the 3D scene interactively."),
            ("Commercial deployment",
             "Apple commercialized related technology as 'Spatial Scenes' in iOS 26. The SHARP code is open-source on GitHub. Splat Studio is available for Vision Pro users."),
        ],
        "sources": [
            "https://apple.github.io/ml-sharp/",
            "https://www.uploadvr.com/apple-sharp-open-source-on-device-gaussian-splatting/",
        ],
        "commentary": "Spatial Scenes in iOS 26 represents commercial deployment across every iPhone, significantly increasing the value of any licensing claim.",
    },
])


# =====================================================================
# AMAZON
# =====================================================================
build("EoU_Amazon.pptx", "Amazon.com, Inc.", "Amazon 3D/AR (Seller App + View in 3D + AWS)", [
    {
        "claim_language": "Obtaining a 2D image, classifying the object, and segmenting the object from the image",
        "mapping": [
            ("obtaining a 2D image of an object",
             "The Amazon Seller App 'Create 3D Models' feature captures product images via the iOS device camera. Amazon requires 2 to 10 reference photos to be submitted with accurate product dimensions for 3D model creation."),
            ("classifying the object in the image",
             "Amazon explicitly classifies products by category for 3D eligibility: 'Eligibility depends on product category, physical properties, and other factors.' Different 3D experiences are applied per category: Virtual Try-On for shoes and eyewear, View in Your Room for furniture, and View in 3D for general products."),
            ("segmenting the object from the image",
             "AWS Spatial Computing blog explicitly states that their 3D reconstruction pipeline includes 'image segmentation and semantic labelling' using Amazon SageMaker and Amazon Bedrock."),
        ],
        "sources": [
            "https://sell.amazon.com/tools/3d-ar",
            "https://aws.amazon.com/blogs/spatial/3d-gaussian-splatting-performant-3d-scene-reconstruction-at-scale/",
        ],
        "commentary": "AWS's own technical blog explicitly documents 'image segmentation' as part of the pipeline. This is a direct public admission of the claim element.",
    },
    {
        "claim_language": "Dimensionally sampling and generating a 3D mesh model (Claim 1) using a trained neural network (Claim 5)",
        "mapping": [
            ("dimensionally sampling the segmented image of the object",
             "Amazon requires 'accurate product dimensions' to be submitted with every 3D model. The Seller App scanning process extracts 3D spatial data over a 5-10 minute session. AWS Human Mesh Recovery uses ScoreHMR with diffusion models to 'capture and reconstruct body parameters from input images.'"),
            ("generating a 3D mesh model based at least on the dimensional sampling",
             "Amazon requires all 3D models to be submitted in GLB or GLTF format. Both are mesh formats by specification, containing vertices, faces, normals, and materials. Amazon mandated 3D models over 360-degree images starting December 14, 2023."),
            ("the 3D mesh model is generated by a trained neural network",
             "AWS ScoreHMR is a diffusion-based neural network. Amazon's certified provider Nextech3D.ai uses neural networks to generate 3D models from 2D images. Amazon's own scanning pipeline uses AI processing."),
        ],
        "sources": [
            "https://sell.amazon.com/tools/3d-ar",
            "https://aws.amazon.com/blogs/spatial/from-2d-to-3d-building-a-scalable-human-mesh-recovery-pipeline-with-amazon-sagemaker-ai/",
        ],
        "commentary": "GLB and GLTF are mesh formats by definition. Amazon mandated these formats platform-wide in December 2023, eliminating any ambiguity about the output format.",
    },
    {
        "claim_language": "Extracting and rendering texture (Claim 1), primary face texture (Claim 7), and multiple views with user input (Claims 9, 10)",
        "mapping": [
            ("extracting texture information and rendering it onto the 3D mesh model",
             "Amazon's 3D product models display photorealistic textures including product labels, colors, branding, and material properties. The GLB and GLTF formats embed texture maps by specification."),
            ("the texture information represents a primary face of the object",
             "Product reference photos capture the front or primary face of the product. This texture information is extracted and rendered onto the 3D mesh for the product listing."),
            ("manipulating the 3D representation to provide multiple different views, based on user input",
             "View in 3D allows customers to 'rotate and zoom in on a product in 3D from all angles.' View in Your Room renders the textured 3D product in AR within the customer's space. Virtual Try-On renders products directly on the user's body for shoes and eyewear."),
        ],
        "sources": [
            "https://sell.amazon.com/tools/3d-ar",
        ],
        "commentary": "Amazon offers three distinct 3D rendering experiences, each satisfying the texture rendering and user manipulation claims. Amazon's own data shows 2X purchase conversion and 20% fewer returns from these features.",
    },
])

print("\nAll 4 EoU decks created (v6 — full sentences, all links).")
