"""
EoU v5 — Clean, no-frills. No page numbers, no arrows, no decorative formatting.
Just claim language and evidence mapping. Business only.
"""

from pptx import Presentation
from pptx.util import Inches, Pt
from pptx.dml.color import RGBColor
from pptx.enum.text import PP_ALIGN
from pptx.enum.shapes import MSO_SHAPE
import os

OUTPUT_DIR = os.path.dirname(os.path.abspath(__file__))
PATENT = "WO/2025/193512A1"

BLACK = RGBColor(0,0,0)
DARK_GRAY = RGBColor(60,60,60)
BLUE = RGBColor(0,50,150)


def add_text(slide, left, top, w, h, text, size=11, bold=False, color=BLACK):
    box = slide.shapes.add_textbox(Inches(left), Inches(top), Inches(w), Inches(h))
    tf = box.text_frame
    tf.word_wrap = True
    p = tf.paragraphs[0]
    p.text = text
    p.font.size = Pt(size)
    p.font.bold = bold
    p.font.color.rgb = color
    return box


def add_lines(slide, left, top, w, h, lines, size=10, color=BLACK):
    box = slide.shapes.add_textbox(Inches(left), Inches(top), Inches(w), Inches(h))
    tf = box.text_frame
    tf.word_wrap = True
    for i, (text, sz, clr, bld) in enumerate(lines):
        if i == 0:
            p = tf.paragraphs[0]
        else:
            p = tf.add_paragraph()
        p.text = text
        p.font.size = Pt(sz)
        p.font.color.rgb = clr
        p.font.bold = bld
        p.space_after = Pt(3)
    return box


def screenshot_box(slide, left, top, w, h):
    shape = slide.shapes.add_shape(MSO_SHAPE.RECTANGLE, Inches(left), Inches(top), Inches(w), Inches(h))
    shape.fill.solid()
    shape.fill.fore_color.rgb = RGBColor(248, 248, 250)
    shape.line.color.rgb = RGBColor(210, 210, 215)
    shape.line.width = Pt(0.5)


def build(filename, company, product, slides_data):
    prs = Presentation()
    prs.slide_width = Inches(10)
    prs.slide_height = Inches(7.5)

    # ============ TITLE SLIDE ============
    sl = prs.slides.add_slide(prs.slide_layouts[6])
    add_text(sl, 0.5, 0.5, 9, 0.4, f"Evidence of Use: {PATENT}", size=12, color=DARK_GRAY)
    add_text(sl, 0.5, 1.1, 9, 0.6, company, size=24, bold=True)
    add_text(sl, 0.5, 1.8, 9, 0.4, product, size=13, color=DARK_GRAY)

    add_text(sl, 0.5, 2.6, 9, 0.3, "CLAIM 1", size=10, bold=True)
    claim1 = (
        '"A method for generating a 3D model comprising: obtaining a 2D image of an object; '
        'classifying the object in the image; segmenting the object from the image; '
        'dimensionally sampling the segmented image of the object; extracting texture information from the segmented image; '
        'generating a 3D mesh model based at least on the dimensional sampling; '
        'and rendering the texture information onto the 3D mesh model to create a 3D representation of the object."'
    )
    add_text(sl, 0.5, 2.95, 9, 1.5, claim1, size=10, color=DARK_GRAY)

    add_text(sl, 0.5, 4.6, 9, 0.3, "DEPENDENT CLAIMS", size=10, bold=True)
    deps = (
        "5: 3D mesh generated by trained neural network\n"
        "7: Texture represents primary face of object\n"
        "8: Primary face texture rendered on opposite sides\n"
        "9: Manipulating 3D to provide multiple views\n"
        "10: Manipulation based on user input\n"
        "11: Capturing 2D image of each manipulation"
    )
    add_text(sl, 0.5, 4.95, 9, 1.5, deps, size=10, color=DARK_GRAY)

    add_text(sl, 0.5, 6.6, 5, 0.25, "Carnegie Mellon University / Savvides et al.", size=9, color=RGBColor(130,130,130))

    # ============ ELEMENT SLIDES ============
    for sd in slides_data:
        sl = prs.slides.add_slide(prs.slide_layouts[6])

        # Claim language at top
        add_text(sl, 0.4, 0.4, 9.2, 0.7, sd["claim_language"], size=11, bold=True)

        # Mappings
        y = 1.2
        for claim_phrase, product_evidence in sd["mapping"]:
            add_text(sl, 0.4, y, 5.5, 0.25, f'"{claim_phrase}"', size=9, bold=True, color=DARK_GRAY)
            add_text(sl, 0.4, y + 0.28, 5.5, 0.5, product_evidence, size=9)
            y += 0.85

        # Screenshot areas (right side)
        screenshot_box(sl, 6.1, 1.2, 3.5, 2.2)
        screenshot_box(sl, 6.1, 3.6, 3.5, 1.8)

        # Sources
        src_y = max(y + 0.1, 5.6)
        add_text(sl, 0.4, src_y, 1, 0.2, "Source:", size=9, bold=True, color=DARK_GRAY)
        for j, src in enumerate(sd["sources"]):
            add_text(sl, 1.2, src_y + j * 0.22, 8.4, 0.22, src, size=8, color=BLUE)

        # Commentary
        if "commentary" in sd and sd["commentary"]:
            comm_y = src_y + len(sd["sources"]) * 0.22 + 0.2
            add_text(sl, 0.4, comm_y, 9.2, 0.8, sd["commentary"], size=9, color=DARK_GRAY)

    path = os.path.join(OUTPUT_DIR, filename)
    prs.save(path)
    print(f"{filename} — {len(slides_data) + 1} slides")


# =====================================================================
# GOOGLE
# =====================================================================
build("EoU_Google_Shopping.pptx", "Google (Alphabet)", "Google Shopping 3D Product Viewer", [
    {
        "claim_language": "obtaining a 2D image; classifying the object; segmenting the object from the image",
        "mapping": [
            ("obtaining a 2D image of an object", "Accepts 1-3 product images as input."),
            ("classifying the object in the image", "Merchant Center requires product category. Veo trained by category."),
            ("segmenting the object from the image", "Blog: 'removing unwanted backgrounds.'"),
        ],
        "sources": ["https://research.google/blog/bringing-3d-shoppable-products-online-with-generative-ai/"],
        "commentary": "Google's blog documents background removal and category-based training.",
    },
    {
        "claim_language": "dimensionally sampling; generating a 3D mesh model; Claim 5: trained neural network",
        "mapping": [
            ("dimensionally sampling", "Gen 1: 3D priors. Gen 2: SDS optimization. Gen 3: Veo multi-angle training."),
            ("generating a 3D mesh model", "Produces 3D representations. Supports glTF/glb mesh formats."),
            ("trained neural network", "Veo, NeRF, diffusion models are all neural networks."),
        ],
        "sources": ["https://research.google/blog/bringing-3d-shoppable-products-online-with-generative-ai/"],
        "commentary": "Three documented generations of neural network pipelines.",
    },
    {
        "claim_language": "extracting texture; rendering texture onto 3D mesh; Claims 9-11: views, user input, 2D capture",
        "mapping": [
            ("extracting texture information", "Veo captures 'light, material, texture, and geometry.'"),
            ("rendering texture onto the 3D mesh", "Photorealistic 360° product spin on Google Shopping."),
            ("multiple views / user input / 2D capture", "Interactive 360° viewer. Users drag to rotate. Each frame = 2D capture."),
        ],
        "sources": ["https://support.google.com/merchants/answer/13671720"],
        "commentary": "Blog explicitly names 'texture.' 360° viewer satisfies Claims 9, 10, 11.",
    },
])


# =====================================================================
# MESHY
# =====================================================================
build("EoU_Meshy_AI.pptx", "Meshy Inc.", "Meshy AI Image-to-3D Generator", [
    {
        "claim_language": "obtaining a 2D image; classifying the object; segmenting the object",
        "mapping": [
            ("obtaining a 2D image of an object", "Single image upload is the primary feature."),
            ("classifying the object in the image", "Generates front/side/back views, requires object type understanding."),
            ("segmenting the object from the image", "Output models are isolated objects, no background."),
        ],
        "sources": ["https://www.meshy.ai/features/image-to-3d"],
        "commentary": "Clean output proves segmentation. Multi-view proves classification.",
    },
    {
        "claim_language": "dimensionally sampling; generating a 3D mesh model; Claim 5: neural network",
        "mapping": [
            ("dimensionally sampling", "Smart Remesh: 1k-300k triangles, configurable sampling."),
            ("generating a 3D mesh model", "Exports FBX, GLB, OBJ, STL, USDZ, BLEND. All mesh formats."),
            ("trained neural network", "Entirely AI/neural network-based pipeline."),
        ],
        "sources": ["https://www.meshy.ai/"],
        "commentary": "OBJ, FBX, GLB, STL are mesh formats by definition.",
    },
    {
        "claim_language": "extracting texture; rendering texture; Claims 7-8: primary face, opposite sides",
        "mapping": [
            ("extracting texture information", "Full PBR: Diffuse, Roughness, Metallic, Normal maps."),
            ("rendering texture onto the 3D mesh", "PBR textures applied to mesh, shown in viewer."),
            ("primary face / opposite sides", "Input = front face texture. Back inferred/mirrored."),
        ],
        "sources": ["https://www.meshy.ai/features/ai-texturing"],
        "commentary": "PBR maps = comprehensive texture extraction. Claims 7-8 observable.",
    },
    {
        "claim_language": "Claims 9-11: multiple views; user input; capturing 2D",
        "mapping": [
            ("multiple different views", "3D viewer shows any angle. Auto-generates front/side/back."),
            ("manipulated based on user input", "Drag to rotate, scroll to zoom."),
            ("capturing a 2D image", "Export renders 2D from any viewpoint."),
        ],
        "sources": ["https://www.meshy.ai/"],
        "commentary": "Viewer + auto-views + export = Claims 9, 10, 11.",
    },
])


# =====================================================================
# APPLE
# =====================================================================
build("EoU_Apple_SHARP.pptx", "Apple Inc.", "SHARP + Spatial Scenes (iOS 26)", [
    {
        "claim_language": "obtaining a 2D image; segmenting the object; dimensionally sampling",
        "mapping": [
            ("obtaining a 2D image of an object", "'Given a single photograph.' CLI: sharp predict -i image.png"),
            ("segmenting the object from the image", "Depth Pro separates foreground/background by depth."),
            ("dimensionally sampling", "Predicts metric-scale 3D Gaussian positions, scales, rotations."),
        ],
        "sources": ["https://github.com/apple/ml-sharp", "https://machinelearning.apple.com/research/sharp-monocular-view"],
        "commentary": "Documented in Apple's GitHub and research page.",
    },
    {
        "claim_language": "extracting texture; generating 3D model; rendering; Claim 5: neural network",
        "mapping": [
            ("extracting texture information", "Spherical harmonic coefficients encode color/appearance."),
            ("generating a 3D mesh model", "OUTPUTS GAUSSIAN SPLATS (.ply), NOT MESH. Claim construction issue."),
            ("rendering texture onto 3D model", "Photorealistic novel views at 100+ FPS."),
            ("trained neural network", "Single forward pass through neural network."),
        ],
        "sources": ["https://huggingface.co/papers/2512.10685"],
        "commentary": "Element (f) risk: Gaussians are not mesh vertices/faces.",
    },
    {
        "claim_language": "Claims 9-10: multiple views; user input; Commercial deployment",
        "mapping": [
            ("multiple views", "Renders novel views from nearby viewpoints."),
            ("user input", "Vision Pro: head movement. Splat Studio: interactive navigation."),
            ("Commercial", "iOS 26 Spatial Scenes. Open-source GitHub. Splat Studio on Vision Pro."),
        ],
        "sources": ["https://apple.github.io/ml-sharp/"],
        "commentary": "iOS 26 = deployed on every iPhone.",
    },
])


# =====================================================================
# AMAZON
# =====================================================================
build("EoU_Amazon.pptx", "Amazon.com, Inc.", "Amazon 3D/AR (Seller App + View in 3D + AWS)", [
    {
        "claim_language": "obtaining a 2D image; classifying the object; segmenting the object",
        "mapping": [
            ("obtaining a 2D image of an object", "Seller App captures images. 2-10 photos required."),
            ("classifying the object in the image", "'Eligibility depends on product category.' Different 3D per category."),
            ("segmenting the object from the image", "AWS blog: 'image segmentation and semantic labelling.'"),
        ],
        "sources": ["https://sell.amazon.com/tools/3d-ar", "https://aws.amazon.com/blogs/spatial/3d-gaussian-splatting-performant-3d-scene-reconstruction-at-scale/"],
        "commentary": "AWS explicitly documents 'image segmentation.'",
    },
    {
        "claim_language": "dimensionally sampling; generating 3D mesh; Claim 5: neural network",
        "mapping": [
            ("dimensionally sampling", "'Accurate product dimensions' required. AWS HMR extracts parameters."),
            ("generating a 3D mesh model", "All models must be GLB/GLTF. Mesh formats by specification."),
            ("trained neural network", "AWS ScoreHMR is neural network-based."),
        ],
        "sources": ["https://sell.amazon.com/tools/3d-ar"],
        "commentary": "GLB/GLTF are mesh formats. Mandated Dec 2023.",
    },
    {
        "claim_language": "extracting texture; rendering; Claims 7, 9, 10: primary face, views, user input",
        "mapping": [
            ("extracting/rendering texture", "3D shows labels, colors, materials. GLB embeds textures."),
            ("primary face texture", "Product photos = front face, rendered on mesh."),
            ("multiple views / user input", "View in 3D: rotate. View in Your Room: AR. Virtual Try-On."),
        ],
        "sources": ["https://sell.amazon.com/tools/3d-ar"],
        "commentary": "Three rendering modes. 2X conversion, 20% fewer returns.",
    },
])

print("\nAll 4 EoU decks created (v5 — clean).")
