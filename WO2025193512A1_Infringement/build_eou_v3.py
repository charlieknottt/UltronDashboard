"""
EoU v3 — Maps exact patent claim language to specific product evidence.
Each slide: verbatim claim text on top, then point-by-point mapping of
each phrase in the claim to the corresponding product behavior.
"""

from pptx import Presentation
from pptx.util import Inches, Pt
from pptx.dml.color import RGBColor
from pptx.enum.text import PP_ALIGN
from pptx.enum.shapes import MSO_SHAPE
import os

OUTPUT_DIR = os.path.dirname(os.path.abspath(__file__))
PATENT = "WO/2025/193512A1"

# Exact verbatim claim text
CLAIM1_FULL = (
    'Claim 1: "A method for generating a 3D model comprising: '
    'obtaining a 2D image of an object; '
    'classifying the object in the image; '
    'segmenting the object from the image; '
    'dimensionally sampling the segmented image of the object; '
    'extracting texture information from the segmented image; '
    'generating a 3D mesh model based at least on the dimensional sampling; '
    'and rendering the texture information onto the 3D mesh model '
    'to create a 3D representation of the object."'
)

CLAIM5 = 'Claim 5: "...wherein the 3D mesh model of the object is generated by a trained neural network based on an input of the at least the dimensional samples."'
CLAIM7 = 'Claim 7: "...wherein the texture information represents a primary face of the object."'
CLAIM8 = 'Claim 8: "...wherein the texture information representing the primary face of the object is rendered on opposite sides of the 3D mesh model."'
CLAIM9 = 'Claim 9: "...further comprising: manipulating the 3D representation to provide multiple different views of the object."'
CLAIM10 = 'Claim 10: "...wherein the 3D representation is manipulated based on user input."'
CLAIM11 = 'Claim 11: "...further comprising: capturing a 2D image of each manipulation of the 3D representation."'


WHITE = RGBColor(255,255,255)
BLACK = RGBColor(0,0,0)
DARK = RGBColor(20,20,45)
BLUE = RGBColor(0,70,170)
GRAY = RGBColor(100,100,110)
RED = RGBColor(180,0,0)
CLAIM_COLOR = RGBColor(100,40,10)  # dark brown for claim text


def add_text(slide, left, top, w, h, text, size=11, bold=False, color=BLACK, align=PP_ALIGN.LEFT, italic=False):
    box = slide.shapes.add_textbox(Inches(left), Inches(top), Inches(w), Inches(h))
    tf = box.text_frame
    tf.word_wrap = True
    p = tf.paragraphs[0]
    p.text = text
    p.font.size = Pt(size)
    p.font.bold = bold
    p.font.italic = italic
    p.font.color.rgb = color
    p.alignment = align
    return box


def add_multi(slide, left, top, w, h, lines, size=9, color=BLACK, line_spacing=Pt(4)):
    box = slide.shapes.add_textbox(Inches(left), Inches(top), Inches(w), Inches(h))
    tf = box.text_frame
    tf.word_wrap = True
    for i, (text, sz, clr, bld) in enumerate(lines):
        if i == 0:
            p = tf.paragraphs[0]
        else:
            p = tf.add_paragraph()
        p.text = text
        p.font.size = Pt(sz)
        p.font.color.rgb = clr
        p.font.bold = bld
        p.space_after = line_spacing
    return box


def screenshot_box(slide, left, top, w, h):
    shape = slide.shapes.add_shape(MSO_SHAPE.RECTANGLE, Inches(left), Inches(top), Inches(w), Inches(h))
    shape.fill.solid()
    shape.fill.fore_color.rgb = RGBColor(242, 242, 246)
    shape.line.color.rgb = RGBColor(195, 195, 205)
    shape.line.dash_style = 2
    shape.line.width = Pt(0.75)
    tf = shape.text_frame
    p = tf.paragraphs[0]
    p.text = "[Screenshot]"
    p.font.size = Pt(8)
    p.font.color.rgb = RGBColor(165, 165, 175)
    p.font.italic = True
    p.alignment = PP_ALIGN.CENTER


def footer(slide, num, total):
    add_text(slide, 0.3, 7.05, 2, 0.2, "CONFIDENTIAL", size=7, bold=True, color=RED)
    add_text(slide, 8.2, 7.05, 1.5, 0.2, f"{num}/{total}", size=7, color=GRAY, align=PP_ALIGN.RIGHT)


def build(filename, company, product, slides_data):
    prs = Presentation()
    prs.slide_width = Inches(10)
    prs.slide_height = Inches(7.5)
    total = len(slides_data) + 1

    # Title slide
    sl = prs.slides.add_slide(prs.slide_layouts[6])
    bar = sl.shapes.add_shape(MSO_SHAPE.RECTANGLE, 0, 0, Inches(10), Inches(0.06))
    bar.fill.solid(); bar.fill.fore_color.rgb = DARK; bar.line.fill.background()

    add_text(sl, 0.5, 1.2, 9, 0.4, f"Evidence of Use: {PATENT}", size=11, color=GRAY)
    add_text(sl, 0.5, 1.8, 9, 0.7, company, size=26, bold=True, color=DARK)
    add_text(sl, 0.5, 2.6, 9, 0.4, product, size=13, color=RGBColor(60,60,80))

    # Full Claim 1 on title slide
    add_text(sl, 0.5, 3.5, 9, 0.3, "Independent Claim Mapped:", size=9, bold=True, color=GRAY)
    add_text(sl, 0.5, 3.8, 9, 1.6, CLAIM1_FULL, size=9, italic=True, color=CLAIM_COLOR)

    add_text(sl, 0.5, 5.6, 9, 0.3, "Dependent Claims Mapped:", size=9, bold=True, color=GRAY)
    deps = "Claims 5 (neural network), 7 (primary face texture), 8 (opposite sides), 9 (multiple views), 10 (user input), 11 (2D capture of views)"
    add_text(sl, 0.5, 5.9, 9, 0.4, deps, size=9, italic=True, color=CLAIM_COLOR)

    add_text(sl, 0.5, 6.5, 9, 0.2, "Carnegie Mellon University  |  Savvides et al.", size=9, color=GRAY)
    footer(sl, 1, total)

    # Element slides
    for i, sd in enumerate(slides_data):
        sl = prs.slides.add_slide(prs.slide_layouts[6])
        bar = sl.shapes.add_shape(MSO_SHAPE.RECTANGLE, 0, 0, Inches(10), Inches(0.06))
        bar.fill.solid(); bar.fill.fore_color.rgb = DARK; bar.line.fill.background()

        # Exact claim language at top (brown italic)
        add_text(sl, 0.3, 0.2, 9.4, 0.7, sd["claim_language"], size=10, italic=True, color=CLAIM_COLOR)

        # Mapping lines: claim phrase → product behavior
        lines = []
        for claim_phrase, product_evidence in sd["mapping"]:
            lines.append((f'"{claim_phrase}"', 8, CLAIM_COLOR, True))
            lines.append((f"→  {product_evidence}", 9, BLACK, False))
            lines.append(("", 4, BLACK, False))  # spacer

        add_multi(sl, 0.3, 0.9, 5.2, 3.2, lines, line_spacing=Pt(2))

        # Screenshot area (right side)
        screenshot_box(sl, 5.7, 0.9, 4.0, 2.0)

        # Source URLs
        y_src = 3.1
        add_text(sl, 5.7, y_src, 1, 0.2, "Source:", size=8, bold=True, color=GRAY)
        for src in sd["sources"]:
            y_src += 0.2
            add_text(sl, 5.7, y_src, 4.0, 0.2, src, size=7, color=BLUE)

        # Second screenshot (below)
        screenshot_box(sl, 5.7, y_src + 0.35, 4.0, 1.6)

        # Commentary at bottom
        if "commentary" in sd:
            add_text(sl, 0.3, 5.8, 9.4, 1.0, sd["commentary"], size=8, color=RGBColor(60,60,60))

        footer(sl, i + 2, total)

    path = os.path.join(OUTPUT_DIR, filename)
    prs.save(path)
    print(f"{filename} — {total} slides")


# =====================================================================
# GOOGLE
# =====================================================================
build("EoU_Google_Shopping.pptx", "Google (Alphabet)", "Google Shopping — AI-Generated 3D Product Viewer", [
    {
        "claim_language": 'Claim 1: "...obtaining a 2D image of an object; classifying the object in the image; segmenting the object from the image..."',
        "mapping": [
            ("obtaining a 2D image of an object",
             "Google accepts as few as 1 product image as input to their 3D generation pipeline."),
            ("classifying the object in the image",
             "Google Merchant Center requires product category assignment. Veo model fine-tuned on millions of assets organized by product category (shoes, furniture, electronics, apparel)."),
            ("segmenting the object from the image",
             "Google's research blog: pipeline includes 'removing unwanted backgrounds' — explicit object segmentation."),
        ],
        "sources": [
            "https://research.google/blog/bringing-3d-shoppable-products-online-with-generative-ai/",
            "https://support.google.com/merchants/answer/6324436",
        ],
        "commentary": "Google's blog directly documents background removal (segmentation). Merchant Center requires explicit product categorization (classification). Both are public admissions.",
    },
    {
        "claim_language": 'Claim 1: "...dimensionally sampling the segmented image of the object; generating a 3D mesh model based at least on the dimensional sampling..."  +  Claim 5: "...generated by a trained neural network..."',
        "mapping": [
            ("dimensionally sampling the segmented image",
             "Gen 1: 'predicting 3D priors' and 'estimating camera positions from sparse images.' Gen 2: Score Distillation Sampling optimizes 3D model parameters. Gen 3: Veo fine-tuned on assets 'rendered from various camera angles.'"),
            ("generating a 3D mesh model based at least on the dimensional sampling",
             "All three generations produce 3D representations (NeRF, optimized mesh, or volumetric) from the extracted spatial data. Google also supports merchant-uploaded glTF/glb (mesh formats)."),
            ("generated by a trained neural network",
             "Veo is a neural network (video diffusion model). NeRF is a neural network. Score Distillation uses a neural diffusion model. All three generations are neural network-based."),
        ],
        "sources": [
            "https://research.google/blog/bringing-3d-shoppable-products-online-with-generative-ai/",
        ],
        "commentary": "Three generations of documented neural network pipelines, each extracting dimensional information and producing 3D output. Blog contains pipeline diagrams for each generation.",
    },
    {
        "claim_language": (
            'Claim 1: "...extracting texture information from the segmented image; and rendering the texture information onto the 3D mesh model to create a 3D representation..."  +  '
            'Claim 9: "...manipulating the 3D representation to provide multiple different views..."  +  '
            'Claim 10: "...manipulated based on user input."  +  '
            'Claim 11: "...capturing a 2D image of each manipulation..."'
        ),
        "mapping": [
            ("extracting texture information from the segmented image",
             "Blog: Veo captures 'complex interactions between light, material, texture, and geometry' — texture explicitly named."),
            ("rendering the texture information onto the 3D mesh model",
             "Output is photorealistic textured 360° product spin rendered on Google Shopping pages."),
            ("manipulating the 3D representation to provide multiple different views",
             "Interactive 360° viewer: users see product from every angle. Google reports 50% more engagement."),
            ("manipulated based on user input",
             "Users drag/rotate the 3D product view on Google Shopping."),
            ("capturing a 2D image of each manipulation",
             "Each frame of the 360° spin is a 2D capture of the 3D representation at that viewpoint."),
        ],
        "sources": [
            "https://research.google/blog/bringing-3d-shoppable-products-online-with-generative-ai/",
            "https://support.google.com/merchants/answer/13671720",
        ],
        "commentary": "Google's blog uses the word 'texture' explicitly. The 360° interactive viewer satisfies Claims 9, 10, and 11 simultaneously.",
    },
])


# =====================================================================
# MESHY
# =====================================================================
build("EoU_Meshy_AI.pptx", "Meshy Inc.", "Meshy AI — Image-to-3D Model Generator", [
    {
        "claim_language": 'Claim 1: "...obtaining a 2D image of an object; classifying the object in the image; segmenting the object from the image..."',
        "mapping": [
            ("obtaining a 2D image of an object",
             "Meshy Image-to-3D: user uploads a single image. This is the primary product feature."),
            ("classifying the object in the image",
             "System generates coherent front/side/back views — requires understanding object type (shoe vs. chair vs. character) to predict unseen angles correctly."),
            ("segmenting the object from the image",
             "Output 3D models contain only the target object with no background. The object is isolated before mesh generation."),
        ],
        "sources": ["https://www.meshy.ai/", "https://www.meshy.ai/features/image-to-3d"],
        "commentary": "Single-image input is Meshy's core feature. Clean isolated 3D output proves segmentation occurred. Multi-view prediction requires implicit classification.",
    },
    {
        "claim_language": (
            'Claim 1: "...dimensionally sampling the segmented image of the object; generating a 3D mesh model based at least on the dimensional sampling..."  +  '
            'Claim 5: "...generated by a trained neural network based on an input of the at least the dimensional samples."'
        ),
        "mapping": [
            ("dimensionally sampling the segmented image",
             "Smart Remesh: adjustable polygon count from 1k to 300k triangles/quads — different sampling granularities of the object's dimensions."),
            ("generating a 3D mesh model",
             "Explicit mesh output in 7 formats: FBX, GLB, OBJ, STL, 3MF, USDZ, BLEND. These are polygon mesh formats by definition (vertices, faces, edges)."),
            ("generated by a trained neural network",
             "Meshy's pipeline is entirely AI/neural network-based. Image-to-3D uses trained models to predict geometry from 2D input."),
        ],
        "sources": ["https://www.meshy.ai/"],
        "commentary": "Strongest match in the portfolio for 'generating a 3D mesh model.' OBJ, FBX, GLB, STL are unambiguously mesh formats. Zero design-around argument on element (f).",
    },
    {
        "claim_language": (
            'Claim 1: "...extracting texture information from the segmented image; and rendering the texture information onto the 3D mesh model..."  +  '
            'Claim 7: "...the texture information represents a primary face of the object."  +  '
            'Claim 8: "...the texture information representing the primary face...is rendered on opposite sides of the 3D mesh model."'
        ),
        "mapping": [
            ("extracting texture information from the segmented image",
             "Full PBR texture extraction: Diffuse, Roughness, Metallic, and Normal maps generated from input image."),
            ("rendering the texture information onto the 3D mesh model",
             "PBR textures applied to mesh geometry. Browser viewer shows textured model. Downloadable with baked textures."),
            ("texture information represents a primary face of the object",
             "Input image captures the front/primary face of the object. This texture is extracted and applied to the mesh."),
            ("rendered on opposite sides of the 3D mesh model",
             "When back texture is unavailable, Meshy mirrors/infers the primary face texture to the opposite side of the mesh."),
        ],
        "sources": ["https://www.meshy.ai/", "https://www.meshy.ai/features/ai-texturing"],
        "commentary": "PBR pipeline (Diffuse, Roughness, Metallic, Normal) is comprehensive texture extraction. Claims 7 and 8 are directly observable — input image texture mapped to front and inferred on back.",
    },
    {
        "claim_language": (
            'Claim 9: "...manipulating the 3D representation to provide multiple different views..."  +  '
            'Claim 10: "...manipulated based on user input."  +  '
            'Claim 11: "...capturing a 2D image of each manipulation of the 3D representation."'
        ),
        "mapping": [
            ("manipulating the 3D representation to provide multiple different views",
             "Browser-based 3D viewer: model viewable from any angle. Meshy also generates multi-view images (front, side, back) automatically."),
            ("manipulated based on user input",
             "Users drag to rotate, scroll to zoom, click to inspect the 3D model interactively."),
            ("capturing a 2D image of each manipulation",
             "Export functionality captures 2D renders from any viewpoint. Multi-format download includes rendered views."),
        ],
        "sources": ["https://www.meshy.ai/"],
        "commentary": "Interactive 3D viewer + multi-view generation + export = all three dependent claims satisfied.",
    },
])


# =====================================================================
# APPLE
# =====================================================================
build("EoU_Apple_SHARP.pptx", "Apple Inc.", "SHARP + Spatial Scenes (iOS 26) + Vision Pro", [
    {
        "claim_language": 'Claim 1: "...obtaining a 2D image of an object; segmenting the object from the image; dimensionally sampling the segmented image of the object..."',
        "mapping": [
            ("obtaining a 2D image of an object",
             "SHARP: 'Given a single photograph, regresses parameters of a 3D Gaussian representation.' CLI: 'sharp predict -i image_path.png.'"),
            ("segmenting the object from the image",
             "Uses Apple Depth Pro for monocular depth estimation — separates foreground object from background by depth. Per-pixel predictions require object boundary understanding."),
            ("dimensionally sampling the segmented image",
             "Predicts metric-scale 3D Gaussian positions ('absolute scale, supporting metric camera movements'). Each Gaussian has position, scale, and rotation — dimensional data extracted from the 2D image."),
        ],
        "sources": [
            "https://github.com/apple/ml-sharp",
            "https://machinelearning.apple.com/research/sharp-monocular-view",
        ],
        "commentary": "Single-image input, depth-based segmentation, and metric-scale dimensional extraction are all documented in Apple's own GitHub repo and research page.",
    },
    {
        "claim_language": (
            'Claim 1: "...extracting texture information from the segmented image; generating a 3D mesh model based at least on the dimensional sampling; '
            'and rendering the texture information onto the 3D mesh model to create a 3D representation..."  +  '
            'Claim 5: "...generated by a trained neural network..."'
        ),
        "mapping": [
            ("extracting texture information from the segmented image",
             "Each 3D Gaussian has spherical harmonic appearance coefficients encoding color and view-dependent appearance extracted from the input image."),
            ("generating a 3D mesh model based at least on the dimensional sampling",
             "NOTE — CLAIM CONSTRUCTION ISSUE: SHARP outputs 3D Gaussian splats (.ply), not a polygon mesh. Functionally equivalent (3D representation from dimensional data), but mathematically distinct from vertices/faces/edges."),
            ("rendering the texture information onto the 3D mesh model to create a 3D representation",
             "Renders photorealistic novel views at 100+ FPS. 'High-resolution rendering with sharp details and fine structures.'"),
            ("generated by a trained neural network",
             "Single feedforward pass through a neural network. Entire pipeline is a trained neural network — no iterative optimization."),
        ],
        "sources": [
            "https://machinelearning.apple.com/research/sharp-monocular-view",
            "https://huggingface.co/papers/2512.10685",
        ],
        "commentary": "Key vulnerability: Gaussian splats vs. 'mesh model' on element (f). Functional equivalence argument is strong (both are 3D representations), but Gaussians lack vertices/faces. Apple's RealityKit ecosystem does use mesh formats, and splats can be converted to meshes. Patent counsel should evaluate claim construction.",
    },
    {
        "claim_language": (
            'Claim 9: "...manipulating the 3D representation to provide multiple different views..."  +  '
            'Claim 10: "...manipulated based on user input."  +  COMMERCIAL DEPLOYMENT'
        ),
        "mapping": [
            ("manipulating the 3D representation to provide multiple different views",
             "SHARP renders novel views from arbitrary nearby viewpoints. Splat Studio app on Vision Pro enables 3D scene exploration."),
            ("manipulated based on user input",
             "Vision Pro: user physically moves head/body to change viewpoint. Splat Studio: interactive 3D scene navigation."),
            ("Commercial deployment",
             "Apple commercialized related technology as 'Spatial Scenes' in iOS 26 (2025). SHARP is open-source on GitHub. Splat Studio available for Vision Pro."),
        ],
        "sources": [
            "https://apple.github.io/ml-sharp/",
            "https://www.uploadvr.com/apple-sharp-open-source-on-device-gaussian-splatting/",
        ],
        "commentary": "Spatial Scenes in iOS 26 = commercial deployment across every iPhone. Open-source code enables full technical verification of the pipeline.",
    },
])


# =====================================================================
# AMAZON
# =====================================================================
build("EoU_Amazon.pptx", "Amazon.com, Inc.", "Amazon 3D/AR — Seller App + View in 3D + AWS Pipeline", [
    {
        "claim_language": 'Claim 1: "...obtaining a 2D image of an object; classifying the object in the image; segmenting the object from the image..."',
        "mapping": [
            ("obtaining a 2D image of an object",
             "Seller App 'Create 3D Models' captures product images via iOS camera. 2-10 reference photos required for 3D model submission."),
            ("classifying the object in the image",
             "Amazon: 'Eligibility depends on product category.' Different 3D experiences per category — Virtual Try-On (shoes/eyewear), View in Your Room (furniture), View in 3D (general). Explicit product taxonomy required."),
            ("segmenting the object from the image",
             "AWS Spatial Computing blog: pipeline includes 'image segmentation and semantic labelling' via SageMaker and Bedrock."),
        ],
        "sources": [
            "https://sell.amazon.com/tools/3d-ar",
            "https://aws.amazon.com/blogs/spatial/3d-gaussian-splatting-performant-3d-scene-reconstruction-at-scale/",
        ],
        "commentary": "AWS explicitly documents 'image segmentation' in their 3D pipeline — direct public admission. Amazon's category-dependent 3D eligibility is explicit product classification.",
    },
    {
        "claim_language": (
            'Claim 1: "...dimensionally sampling the segmented image of the object; generating a 3D mesh model based at least on the dimensional sampling..."  +  '
            'Claim 5: "...generated by a trained neural network..."'
        ),
        "mapping": [
            ("dimensionally sampling the segmented image",
             "'Accurate product dimensions' required with every 3D submission. Seller App scanning extracts 3D spatial data. AWS HMR: ScoreHMR uses diffusion models to 'capture and reconstruct body parameters from input images.'"),
            ("generating a 3D mesh model",
             "All 3D models must be GLB or GLTF format — mesh formats by specification (vertices, faces, normals, materials). Mandatory since December 14, 2023."),
            ("generated by a trained neural network",
             "AWS pipeline uses ScoreHMR (diffusion-based neural network). Certified provider Nextech3D.ai uses neural networks to generate 3D from 2D. Amazon's own scanning pipeline uses AI processing."),
        ],
        "sources": [
            "https://sell.amazon.com/tools/3d-ar",
            "https://aws.amazon.com/blogs/spatial/from-2d-to-3d-building-a-scalable-human-mesh-recovery-pipeline-with-amazon-sagemaker-ai/",
        ],
        "commentary": "GLB/GLTF ARE mesh formats — no ambiguity. Amazon mandated these formats platform-wide in Dec 2023. AWS documents neural network-based 3D reconstruction.",
    },
    {
        "claim_language": (
            'Claim 1: "...extracting texture information...rendering the texture information onto the 3D mesh model to create a 3D representation..."  +  '
            'Claim 7: "...texture information represents a primary face of the object."  +  '
            'Claim 9: "...manipulating the 3D representation to provide multiple different views..."  +  '
            'Claim 10: "...manipulated based on user input."'
        ),
        "mapping": [
            ("extracting texture information...rendering the texture information onto the 3D mesh model",
             "3D product models display photorealistic textures — product labels, colors, branding, materials preserved. GLB/GLTF format embeds texture maps by specification."),
            ("texture information represents a primary face of the object",
             "Product photos capture the primary/front face. This texture is rendered onto the 3D mesh for the product listing."),
            ("manipulating the 3D representation to provide multiple different views",
             "View in 3D: 'rotate and zoom from all angles.' View in Your Room: AR placement. Virtual Try-On: renders on user's body."),
            ("manipulated based on user input",
             "Users drag to rotate, pinch to zoom, tap to place in AR. Three distinct user-input manipulation modes."),
        ],
        "sources": [
            "https://sell.amazon.com/tools/3d-ar",
        ],
        "commentary": "Three rendering modes (3D viewer, AR room, virtual try-on) each satisfy the texture rendering + manipulation claims. Amazon's own data: 2X purchase conversion, 20% fewer returns from these features.",
    },
])

print("\nAll 4 final EoU decks created (v3).")
