"""
EoU v4 — Improved layout. Claim phrase → product evidence mapping clearly visible.
Larger text areas, better spacing, claim language stands out.
"""

from pptx import Presentation
from pptx.util import Inches, Pt
from pptx.dml.color import RGBColor
from pptx.enum.text import PP_ALIGN
from pptx.enum.shapes import MSO_SHAPE
import os

OUTPUT_DIR = os.path.dirname(os.path.abspath(__file__))
PATENT = "WO/2025/193512A1"

WHITE = RGBColor(255,255,255)
BLACK = RGBColor(0,0,0)
DARK = RGBColor(20,20,45)
BLUE = RGBColor(0,70,170)
GRAY = RGBColor(100,100,110)
LIGHT_GRAY = RGBColor(150,150,150)
RED = RGBColor(180,0,0)
CLAIM_BG = RGBColor(255,250,240)  # warm cream for claim boxes
EVIDENCE_BG = RGBColor(240,248,255)  # light blue for evidence


def add_text(slide, left, top, w, h, text, size=11, bold=False, color=BLACK, align=PP_ALIGN.LEFT, italic=False):
    box = slide.shapes.add_textbox(Inches(left), Inches(top), Inches(w), Inches(h))
    tf = box.text_frame
    tf.word_wrap = True
    p = tf.paragraphs[0]
    p.text = text
    p.font.size = Pt(size)
    p.font.bold = bold
    p.font.italic = italic
    p.font.color.rgb = color
    p.alignment = align
    return box


def add_box_with_text(slide, left, top, w, h, text, size=10, bold=False, color=BLACK, bg_color=None, italic=False):
    """Text box with background fill."""
    shape = slide.shapes.add_shape(MSO_SHAPE.RECTANGLE, Inches(left), Inches(top), Inches(w), Inches(h))
    if bg_color:
        shape.fill.solid()
        shape.fill.fore_color.rgb = bg_color
    else:
        shape.fill.background()
    shape.line.fill.background()

    tf = shape.text_frame
    tf.word_wrap = True
    tf.margin_left = Pt(6)
    tf.margin_right = Pt(6)
    tf.margin_top = Pt(4)
    tf.margin_bottom = Pt(4)
    p = tf.paragraphs[0]
    p.text = text
    p.font.size = Pt(size)
    p.font.bold = bold
    p.font.italic = italic
    p.font.color.rgb = color
    return shape


def screenshot_box(slide, left, top, w, h, label="[Insert Screenshot]"):
    shape = slide.shapes.add_shape(MSO_SHAPE.RECTANGLE, Inches(left), Inches(top), Inches(w), Inches(h))
    shape.fill.solid()
    shape.fill.fore_color.rgb = RGBColor(245, 245, 248)
    shape.line.color.rgb = RGBColor(200, 200, 210)
    shape.line.dash_style = 2
    shape.line.width = Pt(1)
    tf = shape.text_frame
    tf.margin_top = Inches(0.4)
    p = tf.paragraphs[0]
    p.text = label
    p.font.size = Pt(9)
    p.font.color.rgb = RGBColor(150, 150, 160)
    p.font.italic = True
    p.alignment = PP_ALIGN.CENTER
    return shape


def footer(slide, num, total):
    add_text(slide, 0.3, 7.05, 2, 0.2, "CONFIDENTIAL", size=7, bold=True, color=RED)
    add_text(slide, 8.2, 7.05, 1.5, 0.2, f"{num}/{total}", size=7, color=LIGHT_GRAY, align=PP_ALIGN.RIGHT)


def build(filename, company, product, slides_data):
    prs = Presentation()
    prs.slide_width = Inches(10)
    prs.slide_height = Inches(7.5)
    total = len(slides_data) + 1

    # ============ TITLE SLIDE ============
    sl = prs.slides.add_slide(prs.slide_layouts[6])

    # Header bar
    bar = sl.shapes.add_shape(MSO_SHAPE.RECTANGLE, 0, 0, Inches(10), Inches(1.1))
    bar.fill.solid()
    bar.fill.fore_color.rgb = DARK
    bar.line.fill.background()

    add_text(sl, 0.4, 0.15, 9, 0.35, f"Evidence of Use — {PATENT}", size=14, bold=True, color=WHITE)
    add_text(sl, 0.4, 0.55, 9, 0.4, "SINGLE SHOT 3D MODELLING FROM 2D IMAGE", size=11, color=RGBColor(180,180,200))

    # Company/Product
    add_text(sl, 0.4, 1.4, 9, 0.5, company, size=24, bold=True, color=DARK)
    add_text(sl, 0.4, 2.0, 9, 0.35, product, size=12, color=GRAY)

    # Claim 1 box
    add_text(sl, 0.4, 2.7, 9, 0.25, "CLAIM 1 (Independent):", size=9, bold=True, color=DARK)
    claim1_text = (
        '"A method for generating a 3D model comprising: obtaining a 2D image of an object; '
        'classifying the object in the image; segmenting the object from the image; '
        'dimensionally sampling the segmented image of the object; extracting texture information from the segmented image; '
        'generating a 3D mesh model based at least on the dimensional sampling; '
        'and rendering the texture information onto the 3D mesh model to create a 3D representation of the object."'
    )
    add_box_with_text(sl, 0.4, 3.0, 9.2, 1.4, claim1_text, size=9, italic=True, color=RGBColor(80,50,20), bg_color=CLAIM_BG)

    # Dependent claims
    add_text(sl, 0.4, 4.6, 9, 0.25, "DEPENDENT CLAIMS MAPPED:", size=9, bold=True, color=DARK)
    deps = (
        "Claim 5: 3D mesh generated by trained neural network\n"
        "Claim 7: Texture represents primary face of object\n"
        "Claim 8: Primary face texture rendered on opposite sides\n"
        "Claim 9: Manipulating 3D to provide multiple views\n"
        "Claim 10: Manipulation based on user input\n"
        "Claim 11: Capturing 2D image of each manipulation"
    )
    add_text(sl, 0.4, 4.9, 9, 1.3, deps, size=9, color=RGBColor(80,50,20))

    add_text(sl, 0.4, 6.4, 5, 0.2, "Carnegie Mellon University  •  Savvides et al.", size=9, color=LIGHT_GRAY)
    footer(sl, 1, total)

    # ============ ELEMENT SLIDES ============
    for i, sd in enumerate(slides_data):
        sl = prs.slides.add_slide(prs.slide_layouts[6])

        # Thin header bar
        bar = sl.shapes.add_shape(MSO_SHAPE.RECTANGLE, 0, 0, Inches(10), Inches(0.5))
        bar.fill.solid()
        bar.fill.fore_color.rgb = DARK
        bar.line.fill.background()
        add_text(sl, 0.3, 0.1, 9, 0.3, f"{company} — Claim Mapping", size=11, bold=True, color=WHITE)

        # Claim language box (cream background)
        add_box_with_text(sl, 0.3, 0.65, 9.4, 0.7, sd["claim_language"],
                          size=10, italic=True, color=RGBColor(80,50,20), bg_color=CLAIM_BG)

        # Mapping section
        y = 1.5
        for claim_phrase, product_evidence in sd["mapping"]:
            # Claim phrase (quoted, bold)
            add_text(sl, 0.3, y, 9.4, 0.25, f'"{claim_phrase}"', size=9, bold=True, color=RGBColor(100,60,20))
            y += 0.25
            # Arrow + evidence (light blue background)
            add_box_with_text(sl, 0.5, y, 5.3, 0.55, f"→ {product_evidence}",
                              size=9, color=BLACK, bg_color=EVIDENCE_BG)
            y += 0.65

        # Screenshot area (right side, spanning the mapping area)
        screenshot_box(sl, 6.0, 1.5, 3.7, y - 1.5 if y > 2.5 else 1.5, "[Screenshot from source]")

        # Sources
        src_y = max(y + 0.15, 4.0)
        add_text(sl, 0.3, src_y, 1, 0.2, "Sources:", size=8, bold=True, color=GRAY)
        for j, src in enumerate(sd["sources"]):
            add_text(sl, 1.1, src_y + j * 0.2, 8.5, 0.2, src, size=8, color=BLUE)

        # Second screenshot
        ss2_y = src_y + len(sd["sources"]) * 0.2 + 0.15
        screenshot_box(sl, 0.3, ss2_y, 4.5, 1.2, "[Additional evidence screenshot]")

        # Commentary
        if "commentary" in sd and sd["commentary"]:
            add_text(sl, 5.0, ss2_y, 4.7, 1.2, sd["commentary"], size=8, color=RGBColor(70,70,70))

        footer(sl, i + 2, total)

    path = os.path.join(OUTPUT_DIR, filename)
    prs.save(path)
    print(f"{filename} — {total} slides")


# =====================================================================
# GOOGLE
# =====================================================================
build("EoU_Google_Shopping.pptx", "Google (Alphabet)", "Google Shopping — AI-Generated 3D Product Viewer", [
    {
        "claim_language": 'Claim 1: "obtaining a 2D image of an object; classifying the object in the image; segmenting the object from the image"',
        "mapping": [
            ("obtaining a 2D image of an object",
             "Google accepts 1-3 product images as pipeline input."),
            ("classifying the object in the image",
             "Merchant Center requires product category. Veo trained on assets organized by category."),
            ("segmenting the object from the image",
             "Blog: 'removing unwanted backgrounds' — explicit segmentation step."),
        ],
        "sources": [
            "https://research.google/blog/bringing-3d-shoppable-products-online-with-generative-ai/",
        ],
        "commentary": "Google's own blog documents background removal. Merchant Center requires explicit category assignment.",
    },
    {
        "claim_language": 'Claim 1: "dimensionally sampling the segmented image; generating a 3D mesh model" + Claim 5: "generated by a trained neural network"',
        "mapping": [
            ("dimensionally sampling the segmented image",
             "Gen 1: 3D priors + camera positions. Gen 2: SDS optimizes 3D params. Gen 3: Veo trained on multi-angle renders."),
            ("generating a 3D mesh model",
             "All 3 generations produce 3D representations. Supports glTF/glb mesh uploads."),
            ("generated by a trained neural network",
             "Veo, NeRF, and diffusion models are all neural networks."),
        ],
        "sources": [
            "https://research.google/blog/bringing-3d-shoppable-products-online-with-generative-ai/",
        ],
        "commentary": "Three generations documented. Blog has pipeline diagrams for each.",
    },
    {
        "claim_language": 'Claim 1: "extracting texture; rendering texture onto 3D mesh" + Claims 9-11: multiple views, user input, 2D capture',
        "mapping": [
            ("extracting texture information",
             "Veo captures 'light, material, texture, and geometry' — texture explicitly named."),
            ("rendering texture onto the 3D mesh model",
             "Output: photorealistic textured 360° product spin on Google Shopping."),
            ("manipulating to provide multiple views + user input",
             "Interactive 360° viewer. Users drag to rotate. 50% more engagement."),
            ("capturing a 2D image of each manipulation",
             "Each frame of 360° spin = 2D capture of that viewpoint."),
        ],
        "sources": [
            "https://support.google.com/merchants/answer/13671720",
        ],
        "commentary": "Blog explicitly uses 'texture.' 360° viewer satisfies Claims 9, 10, 11.",
    },
])


# =====================================================================
# MESHY
# =====================================================================
build("EoU_Meshy_AI.pptx", "Meshy Inc.", "Meshy AI — Image-to-3D Generator", [
    {
        "claim_language": 'Claim 1: "obtaining a 2D image; classifying the object; segmenting the object from the image"',
        "mapping": [
            ("obtaining a 2D image of an object",
             "Single image upload is the core feature. Primary input method."),
            ("classifying the object in the image",
             "Generates front/side/back views — requires object type understanding."),
            ("segmenting the object from the image",
             "Output models are isolated objects with no background."),
        ],
        "sources": ["https://www.meshy.ai/features/image-to-3d"],
        "commentary": "Clean 3D output proves segmentation. Multi-view generation proves classification.",
    },
    {
        "claim_language": 'Claim 1: "dimensionally sampling; generating a 3D mesh model" + Claim 5: "trained neural network"',
        "mapping": [
            ("dimensionally sampling the segmented image",
             "Smart Remesh: 1k-300k triangles — configurable dimensional sampling."),
            ("generating a 3D mesh model",
             "Exports FBX, GLB, OBJ, STL, USDZ, BLEND — all mesh formats."),
            ("generated by a trained neural network",
             "Entirely AI/neural network-based pipeline."),
        ],
        "sources": ["https://www.meshy.ai/"],
        "commentary": "OBJ, FBX, GLB, STL are mesh formats by definition. Strongest element (f) match.",
    },
    {
        "claim_language": 'Claim 1: "extracting texture; rendering texture" + Claims 7-8: primary face, opposite sides',
        "mapping": [
            ("extracting texture information",
             "Full PBR: Diffuse, Roughness, Metallic, Normal maps."),
            ("rendering texture onto the 3D mesh",
             "PBR textures applied to mesh. Browser viewer shows result."),
            ("texture represents primary face",
             "Input image = front face. Texture extracted from this view."),
            ("rendered on opposite sides",
             "Back texture inferred/mirrored when not visible in input."),
        ],
        "sources": ["https://www.meshy.ai/features/ai-texturing"],
        "commentary": "PBR maps are comprehensive texture extraction. Claims 7-8 directly observable.",
    },
    {
        "claim_language": 'Claims 9-11: "multiple views; user input; capturing 2D of each manipulation"',
        "mapping": [
            ("manipulating to provide multiple different views",
             "3D viewer: any angle. Auto-generates front/side/back views."),
            ("manipulated based on user input",
             "Users drag to rotate, scroll to zoom, click to inspect."),
            ("capturing a 2D image of each manipulation",
             "Export renders 2D from any viewpoint. Download includes views."),
        ],
        "sources": ["https://www.meshy.ai/"],
        "commentary": "Interactive viewer + multi-view generation + export = all 3 claims.",
    },
])


# =====================================================================
# APPLE
# =====================================================================
build("EoU_Apple_SHARP.pptx", "Apple Inc.", "SHARP + Spatial Scenes (iOS 26)", [
    {
        "claim_language": 'Claim 1: "obtaining a 2D image; segmenting the object; dimensionally sampling"',
        "mapping": [
            ("obtaining a 2D image of an object",
             "'Given a single photograph.' CLI: sharp predict -i image.png"),
            ("segmenting the object from the image",
             "Depth Pro separates foreground/background by depth."),
            ("dimensionally sampling the segmented image",
             "Predicts metric-scale 3D Gaussian positions, scales, rotations."),
        ],
        "sources": [
            "https://github.com/apple/ml-sharp",
            "https://machinelearning.apple.com/research/sharp-monocular-view",
        ],
        "commentary": "All documented in Apple's GitHub and research page.",
    },
    {
        "claim_language": 'Claim 1: "extracting texture; generating 3D model; rendering" + Claim 5: "neural network"',
        "mapping": [
            ("extracting texture information",
             "Spherical harmonic coefficients encode color/appearance."),
            ("generating a 3D mesh model",
             "⚠️ Outputs Gaussian splats (.ply), NOT polygon mesh. Claim construction issue."),
            ("rendering texture onto 3D model",
             "Photorealistic novel views at 100+ FPS."),
            ("generated by a trained neural network",
             "Single forward pass through neural network."),
        ],
        "sources": ["https://huggingface.co/papers/2512.10685"],
        "commentary": "Element (f) risk: Gaussians ≠ mesh vertices/faces. Functional equivalence argument available.",
    },
    {
        "claim_language": 'Claims 9-10: "multiple views; user input" + COMMERCIAL DEPLOYMENT',
        "mapping": [
            ("manipulating to provide multiple views",
             "Renders novel views from arbitrary nearby viewpoints."),
            ("manipulated based on user input",
             "Vision Pro: head movement changes view. Splat Studio: interactive navigation."),
            ("Commercial deployment",
             "iOS 26 Spatial Scenes. Open-source on GitHub. Splat Studio on Vision Pro."),
        ],
        "sources": [
            "https://apple.github.io/ml-sharp/",
            "https://www.uploadvr.com/apple-sharp-open-source-on-device-gaussian-splatting/",
        ],
        "commentary": "iOS 26 = commercial deployment on every iPhone.",
    },
])


# =====================================================================
# AMAZON
# =====================================================================
build("EoU_Amazon.pptx", "Amazon.com, Inc.", "Amazon 3D/AR — Seller App + View in 3D + AWS", [
    {
        "claim_language": 'Claim 1: "obtaining a 2D image; classifying the object; segmenting the object"',
        "mapping": [
            ("obtaining a 2D image of an object",
             "Seller App captures images. 2-10 reference photos required."),
            ("classifying the object in the image",
             "'Eligibility depends on product category.' Different 3D per category."),
            ("segmenting the object from the image",
             "AWS blog: 'image segmentation and semantic labelling' in pipeline."),
        ],
        "sources": [
            "https://sell.amazon.com/tools/3d-ar",
            "https://aws.amazon.com/blogs/spatial/3d-gaussian-splatting-performant-3d-scene-reconstruction-at-scale/",
        ],
        "commentary": "AWS explicitly documents 'image segmentation' — direct admission.",
    },
    {
        "claim_language": 'Claim 1: "dimensionally sampling; generating 3D mesh" + Claim 5: "neural network"',
        "mapping": [
            ("dimensionally sampling the segmented image",
             "'Accurate product dimensions' required. AWS HMR extracts body params."),
            ("generating a 3D mesh model",
             "All models must be GLB/GLTF — mesh formats (vertices, faces)."),
            ("generated by a trained neural network",
             "AWS ScoreHMR is neural network-based. Nextech3D.ai uses neural nets."),
        ],
        "sources": [
            "https://sell.amazon.com/tools/3d-ar",
            "https://aws.amazon.com/blogs/spatial/from-2d-to-3d-building-a-scalable-human-mesh-recovery-pipeline-with-amazon-sagemaker-ai/",
        ],
        "commentary": "GLB/GLTF ARE mesh formats. Mandated Dec 2023. No ambiguity.",
    },
    {
        "claim_language": 'Claim 1: "extracting texture; rendering texture" + Claims 7, 9, 10: primary face, views, user input',
        "mapping": [
            ("extracting/rendering texture",
             "3D models show photorealistic labels, colors, materials. GLB embeds textures."),
            ("texture represents primary face",
             "Product photos capture front face → rendered on 3D mesh."),
            ("multiple views + user input",
             "View in 3D: rotate/zoom. View in Your Room: AR. Virtual Try-On: on body."),
        ],
        "sources": ["https://sell.amazon.com/tools/3d-ar"],
        "commentary": "Three rendering modes. 2X conversion, 20% fewer returns.",
    },
])

print("\nAll 4 EoU decks created (v4).")
